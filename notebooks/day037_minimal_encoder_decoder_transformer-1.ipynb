{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35465b08-be5f-4ffd-acb6-33452d3ccc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional, List\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3dcadc-3e35-4f8d-8641-5248d58f7813",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d20fa31b-87cf-4191-82f9-f30315bb6589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e97d0e9-6ec7-41cb-8231-a75ccb9fbdda",
   "metadata": {},
   "source": [
    "# 1. Intuition to attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdb37c-765c-4896-a63d-0512d0ec3adf",
   "metadata": {},
   "source": [
    "## 1.1 What is attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1ac52-c0c0-42ab-b5e7-4f445eccfb8f",
   "metadata": {},
   "source": [
    "我有三份信息（Values, V）要综合，比如 V1, V2, V3；我手上还有一组权重（weights），比如 0.7, 0.2, 0.1（加起来=1）。\n",
    "那综合结果就是：0.7*V1 + 0.2*V2 + 0.1*V3。\n",
    "——这就是注意力结果（只不过真正的权重不是手填，而是“算出来”的）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04a291e-ce4a-4ab3-8adb-a1aaaf0f0c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.5 2.5]\n"
     ]
    }
   ],
   "source": [
    "def weighted_average(values, weights):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    values: np.ndarray, shape [N, D] N informations, each is D-dimentional vector\n",
    "    weights: np.ndarray, shape [N]\n",
    "\n",
    "    Outputs:\n",
    "    out: np.ndarray, shape[D],= weighted average\n",
    "    \"\"\"\n",
    "    weights = weights / (weights.sum() + 1e-12) #normalization\n",
    "    return (weights[:, None] * values).sum(axis=0)\n",
    "\n",
    "V = np.array([[10., 0.],   # V1\n",
    "              [ 0.,10.],   # V2\n",
    "              [ 5., 5.]])  # V3   \n",
    "w = np.array([0.7, 0.2, 0.1]) \n",
    "out = weighted_average(V, w)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20267e2b-0259-4edf-b46d-be99440a766b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.5, 2.5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w @ V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de895e1-ae32-4ef9-b2db-81414a924237",
   "metadata": {},
   "source": [
    "## 1.2 Where is weight from?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadd18ef-3ac5-47a4-8426-888846c902de",
   "metadata": {},
   "source": [
    "想法：\n",
    "\n",
    "* 我有一个“问题”向量 Q（Query），表示“我现在想要什么”。\n",
    "\n",
    "* 还有一堆“候选”向量 K（Keys），每个候选都有对应的 V（Values） 信息。\n",
    "\n",
    "* 打分：用 Q 去和每个 K 做“相似度”（我们用点积，越像分越高）。\n",
    "\n",
    "* 归一化：把这些分数做 softmax（指数归一化）→ 得到 0~1 的权重，且和=1。\n",
    "\n",
    "* 带权平均：用这些权重去加权 V，得到输出。\n",
    "\n",
    "这就是常说的 Scaled Dot-Product Attention（缩放点积注意力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160b0c0f-3686-484e-b42d-bcbcd1918612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    x: np.ndarray, shape[N]\n",
    "\n",
    "    output:\n",
    "    p: np.ndarray, shape[N]\n",
    "\n",
    "    how it works:\n",
    "    p[i] = exp(x[i]) / sum(exp(x[j]))\n",
    "    \"\"\"\n",
    "    x = x - x.max()\n",
    "    e = np.exp(x)\n",
    "    return e / (e.sum() + 1e-12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82abbd63-c74f-4665-a8ab-ce7ccb8f6740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_once(Q, K, V):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    Q: np.ndarary, shape[D] one query vector\n",
    "    K: np.ndarray, shape[N, D] N keys\n",
    "    V: np.ndarray, shape[N, Dv] every key corresponding values\n",
    "\n",
    "    output:\n",
    "    out: np.ndarray, shape [Dv] attention output\n",
    "    weights: np.ndarray, shape [N] attention weights (sum == 1)\n",
    "\n",
    "    how it works:\n",
    "    1) scores[i] = dot(Q, K[i])\n",
    "    2) weights = softmax(scores)\n",
    "    3) out = sum(weights[i] * V[i]) over i\n",
    "    \"\"\"\n",
    "\n",
    "    scores = K @ Q\n",
    "\n",
    "    weights = softmax(scores)\n",
    "\n",
    "    out = weighted_average(V, weights)\n",
    "\n",
    "    return out, weights, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddc13b7d-a396-4460-8d3f-b71234fb83c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "打分 scores : [ 1.   0.7 -1. ]\n",
      "权重 weights: [0.53300543 0.39486013 0.07213444]\n",
      "输出 out    : [5.69072648 4.30927352]\n"
     ]
    }
   ],
   "source": [
    "Q = np.array([1.0, 0.0])                    # I would like the information along x axis\n",
    "K = np.array([[1.0, 0.0],                   # K1, same as Q, along with it\n",
    "              [0.7, 0.2],                   # K2, sort of align with Q\n",
    "              [-1.0, 0.0]])                 # K3, opposite direction of W\n",
    "V = np.array([[10., 0.],                    # V1\n",
    "              [ 0.,10.],                    # V2\n",
    "              [ 5., 5.]], dtype=np.float32) # V3\n",
    "\n",
    "out, w, s = attention_once(Q, K, V)\n",
    "print(\"打分 scores :\", s)       # 看哪个最相关（越大越相关）\n",
    "print(\"权重 weights:\", w)       # softmax 后变为概率分布（和=1）\n",
    "print(\"输出 out    :\", out)     # 带权平均的结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c4013e-eb76-4654-b0e0-de1e6f5256d2",
   "metadata": {},
   "source": [
    "need * 1/sqrt(D) if large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643a0492-158f-442f-b541-a022f923aa3e",
   "metadata": {},
   "source": [
    "## 1.3 What is mask?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00a7e1-346a-47c4-990a-0e6796d8b4e3",
   "metadata": {},
   "source": [
    "* Padding Mask（填充掩码）：补齐出来的 <pad> 位置是“无效的”，权重要变成 0。\n",
    "做法：在分数上把这些位置加上一个超大负数（如 -1e9），softmax 后几乎就是 0。\n",
    "\n",
    "* Causal Mask（因果掩码）：解码时不能看未来（当前位置只能看它前面），把“未来位置”也加上超大负数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bec80276-b589-4151-a097-76f48c1bb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask(scores, mask):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    scores: np.ndarray, shape [N], original score\n",
    "    mask: np.ndarray, shape [N], dtype = bool, True means need to mask that position\n",
    "\n",
    "    output:\n",
    "    masked_score: np.ndarray, shape [N]\n",
    "\n",
    "    how it works:\n",
    "    make mask==True position extremely small, after softmax, it will very close to 0\n",
    "    \"\"\"\n",
    "\n",
    "    masked = scores.copy()\n",
    "    masked[mask] = -1e9\n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81fc3093-5b65-498d-ab0f-74e8cc27c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = np.array([3.0, 1.0, -2.0], dtype=np.float32)\n",
    "mask   = np.array([False, True, False])  # 第二个位置是 pad，要遮住\n",
    "masked_scores = apply_mask(scores, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b414a6-52ab-4b9f-a87e-e2bd41f94149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原分数: [ 3.  1. -2.] -> softmax [0.8756006  0.11849965 0.00589975]\n",
      "打掩码: [ 3.e+00 -1.e+09 -2.e+00] -> softmax [0.9933072  0.         0.00669285]\n"
     ]
    }
   ],
   "source": [
    "print(\"原分数:\", scores, \"-> softmax\", softmax(scores))\n",
    "print(\"打掩码:\", masked_scores, \"-> softmax\", softmax(masked_scores))  # 中间那个几乎 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8aae04-8c63-4e4e-94fe-cfe9021a29ad",
   "metadata": {},
   "source": [
    "## 1.4 Self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5bed55-a648-4aeb-80ec-9c108dfe19a6",
   "metadata": {},
   "source": [
    "关键：现在 Q/K/V 都来自同一句话的向量表示 X。\n",
    "最简单的演示：我们先不做任何线性变换，直接令 Q=K=V=X（现实里会各自过一层线性投影，这里先别管）。\n",
    "\n",
    "这样你能看到：每个词的位置会按相似度从其他词那里“取信息”（加权平均）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab19a071-daf8-4b79-a1a7-e02a97d688c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention_minimal(X):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    X: np.ndarray shape [T, D], one sequence of T vectors (each vector is D-dimensional)\n",
    "\n",
    "    output:\n",
    "    Y: np.ndarray shape [T, D], self attention output, every position i is a new representation from weight averaged all positions\n",
    "\n",
    "    how it works:\n",
    "    for every i:\n",
    "        socres[i, j] = X[i] X[j]\n",
    "        weights[i] = softmax(socres[i])\n",
    "        Y[i] = sum(weights[i, j] * X[j]) over j\n",
    "    \"\"\"\n",
    "\n",
    "    T, D = X.shape\n",
    "    Y = np.zeros_like(X)\n",
    "    for i in range(T):\n",
    "        scores = X @ X[i]\n",
    "        weights = softmax(scores)\n",
    "        Y[i] = weighted_average(X, weights)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63a03c74-4300-4094-8756-1b1b387657bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 X 形状: (4, 3)\n",
      "输出 Y 形状: (4, 3)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.randn(4, 3).astype(np.float32)  # T=4 词，D=3 维\n",
    "Y = self_attention_minimal(X)\n",
    "print(\"输入 X 形状:\", X.shape)\n",
    "print(\"输出 Y 形状:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "573d3b93-cf98-4099-86b7-c4fd6ad3c666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7640524 ,  0.4001572 ,  0.978738  ],\n",
       "       [ 2.2408931 ,  1.867558  , -0.9772779 ],\n",
       "       [ 0.95008844, -0.1513572 , -0.10321885],\n",
       "       [ 0.41059852,  0.14404356,  1.4542735 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8556d5de-a4d8-49dd-a7e9-92f042eafea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.7975295 ,  0.85911125,  0.3104185 ],\n",
       "       [ 2.238525  ,  1.8615676 , -0.9702689 ],\n",
       "       [ 1.7368875 ,  0.9578309 , -0.05566131],\n",
       "       [ 1.0923121 ,  0.30132565,  1.0670953 ]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29966d-1d75-4b1d-b1d8-097511b38276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
