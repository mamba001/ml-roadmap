{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b885ae9-a1a5-4d03-b6e7-76efea453b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10ab9cf90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, math, json, random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "from transformers import BertConfig, BertForSequenceClassification, AutoModel\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008efae-c6b6-4b5b-af8e-8739df348ba4",
   "metadata": {},
   "source": [
    "# 1. AutoTokenizer basics (padding/truncation/stride/offsets/sentence pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8298e-1e8b-4ee0-9d14-94574abe930d",
   "metadata": {},
   "source": [
    "# 1.1 padding/truncation/offsets/tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef20be38-0357-4063-a8d2-59eb30076de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9360ba5b6e2944589cb2baba30123cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaa3b20bdd14a52876bd8a0c85bc01a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbb6e2979bd456d9c175aae32b323eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6975c497065e4001bf761cb5a68b0723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.1] keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])\n",
      "[1.1] input_ids shape: torch.Size([1, 16])\n",
      "[1.1] attention_mask shape: torch.Size([1, 16])\n",
      "[1.1] offsets shape: torch.Size([1, 16, 2])\n",
      "[1.1] tokens: ['[CLS]', 'hugging', 'face', 'token', '##zier', '##s', 'make', 'sub', '##words', 'easy', '&', 'fast', '!', '[SEP]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "name = \"bert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(name, use_fast=True) #Fast tokenizer (Rust backend)\n",
    "\n",
    "text = \"Hugging Face tokenziers make subwords easy & fast!\"\n",
    "batch = tok(\n",
    "    text,                           # input: string or list of strings\n",
    "    padding=\"max_length\",           # pad to max_length\n",
    "    truncation=True,                # truncate if longer than max_length\n",
    "    max_length=16,                  # set max_length\n",
    "    return_offsets_mapping=True,    # return offset mapping\n",
    "    return_tensors=\"pt\"             # output PyTorch tensors\n",
    ")\n",
    "\n",
    "print(\"\\n[1.1] keys:\", batch.keys())\n",
    "print(\"[1.1] input_ids shape:\", batch[\"input_ids\"].shape)             # [B=1, L]\n",
    "print(\"[1.1] attention_mask shape:\", batch[\"attention_mask\"].shape)   # [1, L]\n",
    "print(\"[1.1] offsets shape:\", batch[\"offset_mapping\"].shape)          # [1, L, 2]\n",
    "print(\"[1.1] tokens:\", tok.convert_ids_to_tokens(batch[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9e9d56e-3554-4264-9554-6be8c844c3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c097458b-6e30-4c39-b9e2-e52bbbc81017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 17662,  2227, 19204, 21548,  2015,  2191,  4942, 22104,  3733,\n",
       "          1004,  3435,   999,   102,     0,     0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "397b7051-0207-4987-801e-d5cf3e093cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hugging',\n",
       " 'face',\n",
       " 'token',\n",
       " '##zier',\n",
       " '##s',\n",
       " 'make',\n",
       " 'sub',\n",
       " '##words',\n",
       " 'easy',\n",
       " '&',\n",
       " 'fast',\n",
       " '!',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_ids_to_tokens(batch[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a80433de-edb3-4b0d-92a6-db1f2c1d261c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72d5fd09-7168-436f-ba92-3811844d1231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ace3e526-6950-4ccc-b317-96a5f34871dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  0],\n",
       "         [ 0,  7],\n",
       "         [ 8, 12],\n",
       "         [13, 18],\n",
       "         [18, 22],\n",
       "         [22, 23],\n",
       "         [24, 28],\n",
       "         [29, 32],\n",
       "         [32, 37],\n",
       "         [38, 42],\n",
       "         [43, 44],\n",
       "         [45, 49],\n",
       "         [49, 50],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"offset_mapping\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685f9765-39cc-4fb3-b4da-6ceaabc100fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1] token[3] spans original text[13:18] -> 'token'\n"
     ]
    }
   ],
   "source": [
    "# visualize some toekn's coverrage\n",
    "i = 3\n",
    "start, end = batch[\"offset_mapping\"][0, i].tolist()\n",
    "print(f\"[1.1] token[{i}] spans original text[{start}:{end}] ->\", repr(text[start:end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef78d47-356f-4e7d-9eb5-6c905861fb36",
   "metadata": {},
   "source": [
    "## 1.2 sentence pair: token_type_ids (segment ids, BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50c2bf75-fc7e-4cc6-97df-0dfc79151bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.2] pair keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[1.2] input_ids: tensor([[  101,  1037,  4937,  7719,  2006,  1996, 13523,  1012,   102,  2019,\n",
      "          4111,  2003,  8345, 24274,  1012,   102]])\n",
      "[1.2] token_type_ids: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])\n",
      "[1.2] tokens: ['[CLS]', 'a', 'cat', 'sits', 'on', 'the', 'mat', '.', '[SEP]', 'an', 'animal', 'is', 'resting', 'indoors', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "premise = \"A cat sits on the mat.\"\n",
    "hypo = \"An animal is resting indoors.\"\n",
    "pair = tok(\n",
    "    premise,\n",
    "    hypo,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    return_token_type_ids=True,  #BERT sentence pair differentiation\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"\\n[1.2] pair keys:\", pair.keys())\n",
    "print(\"[1.2] input_ids:\", pair[\"input_ids\"])\n",
    "print(\"[1.2] token_type_ids:\", pair[\"token_type_ids\"])  # 0=sentence A, 1=sentence B\n",
    "print(\"[1.2] tokens:\", tok.convert_ids_to_tokens(pair[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb115cd9-9aff-49b6-afe2-8f8ebd7829ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1037,  4937,  7719,  2006,  1996, 13523,  1012,   102,  2019,\n",
       "          4111,  2003,  8345, 24274,  1012,   102]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69181dc7-38d3-4d17-b594-ae0c399b04e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'a',\n",
       " 'cat',\n",
       " 'sits',\n",
       " 'on',\n",
       " 'the',\n",
       " 'mat',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'an',\n",
       " 'animal',\n",
       " 'is',\n",
       " 'resting',\n",
       " 'indoors',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.convert_ids_to_tokens(pair[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b50fb5-c4a9-4a05-9492-c2e293cc0d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[\"token_type_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f12dd8-e660-4f60-a0d7-cc37c70be2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pair[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb2e03-09b0-4e11-8b35-a495b7583174",
   "metadata": {},
   "source": [
    "## 1.3 long text sliding window: stride + overflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30b9a8db-488c-4d33-ba69-5eae0f35fa3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful. Transformers are powerful.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_text = \" \".join([\"Transformers are powerful.\"] * 20)\n",
    "long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7667da87-e02a-4a32-a241-5f09a6cae1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1.3] num_overflowing_slices: 8\n",
      "[1.3] shapes: ids torch.Size([8, 16]) offsets torch.Size([8, 16, 2])\n",
      "[1.3] overflow_to_sample_mapping: tensor([0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "pack = tok(\n",
    "    long_text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=16,\n",
    "    stride=4,                           # overlap 4 tokens between windown\n",
    "    return_overflowing_tokens=True,     # return multiple slices\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"\\n[1.3] num_overflowing_slices:\", len(pack[\"input_ids\"]))      # [N_slices, L]\n",
    "print(\"[1.3] shapes: ids\", pack[\"input_ids\"].shape, \"offsets\", pack[\"offset_mapping\"].shape)\n",
    "print(\"[1.3] overflow_to_sample_mapping:\", pack[\"overflow_to_sample_mapping\"])  # which sample is every slice from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34f75829-2d02-452e-99c9-9699368b8d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0cfe865-d263-4224-9ea0-8442d9af9975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 19081,  2024,  3928,  1012, 19081,  2024,  3928,  1012, 19081,\n",
       "          2024,  3928,  1012, 19081,  2024,   102],\n",
       "        [  101,  3928,  1012, 19081,  2024,  3928,  1012, 19081,  2024,  3928,\n",
       "          1012, 19081,  2024,  3928,  1012,   102],\n",
       "        [  101, 19081,  2024,  3928,  1012, 19081,  2024,  3928,  1012, 19081,\n",
       "          2024,  3928,  1012, 19081,  2024,   102],\n",
       "        [  101,  3928,  1012, 19081,  2024,  3928,  1012, 19081,  2024,  3928,\n",
       "          1012, 19081,  2024,  3928,  1012,   102],\n",
       "        [  101, 19081,  2024,  3928,  1012, 19081,  2024,  3928,  1012, 19081,\n",
       "          2024,  3928,  1012, 19081,  2024,   102],\n",
       "        [  101,  3928,  1012, 19081,  2024,  3928,  1012, 19081,  2024,  3928,\n",
       "          1012, 19081,  2024,  3928,  1012,   102],\n",
       "        [  101, 19081,  2024,  3928,  1012, 19081,  2024,  3928,  1012, 19081,\n",
       "          2024,  3928,  1012, 19081,  2024,   102],\n",
       "        [  101,  3928,  1012, 19081,  2024,  3928,  1012, 19081,  2024,  3928,\n",
       "          1012,   102,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23d7522a-ad9b-4841-a60b-75d0278361c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[CLS]',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  '[SEP]'],\n",
       " ['[CLS]',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  'transformers',\n",
       "  'are',\n",
       "  'powerful',\n",
       "  '.',\n",
       "  '[SEP]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]',\n",
       "  '[PAD]']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tok.convert_ids_to_tokens(pack[\"input_ids\"][i]) for i in range(len(pack[\"input_ids\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3c7eeba-4a27-43ae-9d06-9c9b6628ed88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f221733-93e2-4510-91d9-c2866f1751a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6fe9b14-ace7-4bd3-b55c-8dcb55355432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   0],\n",
       "         [  0,  12],\n",
       "         [ 13,  16],\n",
       "         [ 17,  25],\n",
       "         [ 25,  26],\n",
       "         [ 27,  39],\n",
       "         [ 40,  43],\n",
       "         [ 44,  52],\n",
       "         [ 52,  53],\n",
       "         [ 54,  66],\n",
       "         [ 67,  70],\n",
       "         [ 71,  79],\n",
       "         [ 79,  80],\n",
       "         [ 81,  93],\n",
       "         [ 94,  97],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [ 71,  79],\n",
       "         [ 79,  80],\n",
       "         [ 81,  93],\n",
       "         [ 94,  97],\n",
       "         [ 98, 106],\n",
       "         [106, 107],\n",
       "         [108, 120],\n",
       "         [121, 124],\n",
       "         [125, 133],\n",
       "         [133, 134],\n",
       "         [135, 147],\n",
       "         [148, 151],\n",
       "         [152, 160],\n",
       "         [160, 161],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [135, 147],\n",
       "         [148, 151],\n",
       "         [152, 160],\n",
       "         [160, 161],\n",
       "         [162, 174],\n",
       "         [175, 178],\n",
       "         [179, 187],\n",
       "         [187, 188],\n",
       "         [189, 201],\n",
       "         [202, 205],\n",
       "         [206, 214],\n",
       "         [214, 215],\n",
       "         [216, 228],\n",
       "         [229, 232],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [206, 214],\n",
       "         [214, 215],\n",
       "         [216, 228],\n",
       "         [229, 232],\n",
       "         [233, 241],\n",
       "         [241, 242],\n",
       "         [243, 255],\n",
       "         [256, 259],\n",
       "         [260, 268],\n",
       "         [268, 269],\n",
       "         [270, 282],\n",
       "         [283, 286],\n",
       "         [287, 295],\n",
       "         [295, 296],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [270, 282],\n",
       "         [283, 286],\n",
       "         [287, 295],\n",
       "         [295, 296],\n",
       "         [297, 309],\n",
       "         [310, 313],\n",
       "         [314, 322],\n",
       "         [322, 323],\n",
       "         [324, 336],\n",
       "         [337, 340],\n",
       "         [341, 349],\n",
       "         [349, 350],\n",
       "         [351, 363],\n",
       "         [364, 367],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [341, 349],\n",
       "         [349, 350],\n",
       "         [351, 363],\n",
       "         [364, 367],\n",
       "         [368, 376],\n",
       "         [376, 377],\n",
       "         [378, 390],\n",
       "         [391, 394],\n",
       "         [395, 403],\n",
       "         [403, 404],\n",
       "         [405, 417],\n",
       "         [418, 421],\n",
       "         [422, 430],\n",
       "         [430, 431],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [405, 417],\n",
       "         [418, 421],\n",
       "         [422, 430],\n",
       "         [430, 431],\n",
       "         [432, 444],\n",
       "         [445, 448],\n",
       "         [449, 457],\n",
       "         [457, 458],\n",
       "         [459, 471],\n",
       "         [472, 475],\n",
       "         [476, 484],\n",
       "         [484, 485],\n",
       "         [486, 498],\n",
       "         [499, 502],\n",
       "         [  0,   0]],\n",
       "\n",
       "        [[  0,   0],\n",
       "         [476, 484],\n",
       "         [484, 485],\n",
       "         [486, 498],\n",
       "         [499, 502],\n",
       "         [503, 511],\n",
       "         [511, 512],\n",
       "         [513, 525],\n",
       "         [526, 529],\n",
       "         [530, 538],\n",
       "         [538, 539],\n",
       "         [  0,   0],\n",
       "         [  0,   0],\n",
       "         [  0,   0],\n",
       "         [  0,   0],\n",
       "         [  0,   0]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0f877a6-7dad-4059-b870-c8ee8f8aac3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack['overflow_to_sample_mapping']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30840cb1-cfd8-4ad9-8256-f8032e96a266",
   "metadata": {},
   "source": [
    "# 2. Train self-defined BPE, used as fast tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7943c711-d6ed-4899-91c8-e0b13681b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, WordPiece, Unigram\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer, WordPieceTrainer, UnigramTrainer\n",
    "from tokenizers.normalizers import Sequence as NormSeq, NFKC, Lowercase, Strip\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers.decoders import BPEDecoder, WordPiece as WordPieceDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04da7435-6264-49a1-bbdb-02877a3abdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "[2] custom BPE batch keys: dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask', 'offset_mapping'])\n",
      "[2] input_ids shape: torch.Size([2, 16])\n",
      "[2] special_tokens_mask: tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
      "[2] tokens[0]: ['[CLS]', 'tokenizers', 'ar', 'e', 'g', 'r', 'e', 'at', '[UNK]', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"Transformers encode and decode sequences.\",\n",
    "    \"Tokenizers split text into subwords efficiently.\",\n",
    "    \"We can train BPE or WordPiece or Unigram models.\",\n",
    "    \"Hugging Face provides fast tokenization.\",\n",
    "    \"Subword methods handle out-of-vocabulary words.\"\n",
    "]\n",
    "\n",
    "model = BPE(unk_token=\"[UNK]\")\n",
    "# model = WordPiece(unk_token=\"[UNK]\")\n",
    "# model = Unigram()\n",
    "\n",
    "tok_bpe = Tokenizer(model)\n",
    "\n",
    "# normalizer\n",
    "tok_bpe.normalizer = NormSeq([NFKC(), Lowercase(), Strip()])\n",
    "\n",
    "# pretokenizer\n",
    "tok_bpe.pre_tokenizer = Whitespace()\n",
    "\n",
    "# special tokens and trainer\n",
    "specials = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=2000,\n",
    "    min_frequency=1,\n",
    "    special_tokens=specials\n",
    ")\n",
    "# trainer = WordPieceTrainer(vocab_size=2000, special_tokens=specials) # if use WordPiece model\n",
    "# trainer = UnigramTrainer(vocab_size=2000, special_tokens=specials)   # if use Unigram model\n",
    "\n",
    "# train\n",
    "tok_bpe.train_from_iterator(corpus, trainer=trainer)\n",
    "\n",
    "# post-processor (BERT style): [CLS] A [SEP] (optional B) [SEP]\n",
    "tok_bpe.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B [SEP]\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", tok_bpe.token_to_id(\"[CLS]\")),\n",
    "        (\"[SEP]\", tok_bpe.token_to_id(\"[SEP]\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# decoder\n",
    "tok_bpe.decoder = BPEDecoder()\n",
    "\n",
    "# save and load\n",
    "save_dir = \"day040_bpe_tok\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "tok_bpe.save(os.path.join(save_dir, \"tokenizer.json\"))\n",
    "\n",
    "# Wrap (for transformers)\n",
    "fast = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tok_bpe,\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "# try\n",
    "texts = [\"Tokenizers are great!\", \"We train a tiny BPE tokenizer today.\"]\n",
    "batch = fast(\n",
    "    texts,\n",
    "    padding=\"longest\",\n",
    "    truncation=True,\n",
    "    max_length=24,\n",
    "    return_tensors=\"pt\",\n",
    "    return_special_tokens_mask=True,\n",
    "    return_offsets_mapping=True\n",
    ")\n",
    "print(\"\\n[2] custom BPE batch keys:\", batch.keys())\n",
    "print(\"[2] input_ids shape:\", batch[\"input_ids\"].shape)             # [B, L]\n",
    "print(\"[2] special_tokens_mask:\", batch[\"special_tokens_mask\"])\n",
    "print(\"[2] tokens[0]:\", fast.convert_ids_to_tokens(batch[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24db09f2-6279-4a74-8662-7668c91c0a4e",
   "metadata": {},
   "source": [
    "# 3. Aligning tokenizer with a model (vocab_size, special token IDs, resize embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e8181-199d-4abb-afd6-5cf94f2a684b",
   "metadata": {},
   "source": [
    "## 3.1 method1: match vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9918e8fd-31dd-4252-becb-250c4a38fd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[C1] fresh model forward ok; logits shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "vocab_size = fast.backend_tokenizer.get_vocab_size()\n",
    "cfg = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=256,\n",
    "    pad_token_id=fast.pad_token_id\n",
    ")\n",
    "\n",
    "model_fresh = BertForSequenceClassification(cfg)\n",
    "\n",
    "# do a forward once\n",
    "inputs = fast(\n",
    "    [\"just a quick check.\", \"and another sentence.\"],\n",
    "    padding=True, \n",
    "    truncation=True, \n",
    "    max_length=16, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "out = model_fresh(**inputs)   # ok：embeddings 大小与 vocab 对齐\n",
    "print(\"\\n[C1] fresh model forward ok; logits shape:\", out.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e02da-acec-42bb-80ae-780cb6856f1a",
   "metadata": {},
   "source": [
    "## 3.2 load pretrained model, then expand the vocab (need download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6483790-ff80-4375-b87f-dfe3753de609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46bef8c6e878491798d89ad4563d962d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cancellation requested; stopping current tasks.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     base_tok \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# add new token（example）\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     num_added \u001b[38;5;241m=\u001b[39m base_tok\u001b[38;5;241m.\u001b[39madd_tokens([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[NEW_TAG]\u001b[39m\u001b[38;5;124m\"\u001b[39m])        \u001b[38;5;66;03m# return number of newly added\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/transformers/modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/transformers/modeling_utils.py:3854\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3839\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m   3840\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[1;32m   3842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[1;32m   3853\u001b[0m     }\n\u001b[0;32m-> 3854\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3856\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[1;32m   3857\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[1;32m   3858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[1;32m   3859\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/transformers/utils/hub.py:342\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 342\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    357\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    992\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1171\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[0;32m-> 1171\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pointer_path):\n\u001b[1;32m   1184\u001b[0m         _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1723\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[0;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[1;32m   1722\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1723\u001b[0m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants\u001b[38;5;241m.\u001b[39mHF_HUB_DISABLE_XET:\n",
      "File \u001b[0;32m~/Documents/MOOC/ML/env/lib/python3.10/site-packages/huggingface_hub/file_download.py:629\u001b[0m, in \u001b[0;36mxet_get\u001b[0;34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n\u001b[1;32m    627\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(progress_bytes)\n\u001b[0;32m--> 629\u001b[0m \u001b[43mdownload_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxet_download_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconnection_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpiration_unix_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_refresher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_refresher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_updater\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    base_tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "    base_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "    # add new token（example）\n",
    "    num_added = base_tok.add_tokens([\"[NEW_TAG]\"])        # return number of newly added\n",
    "    base_model.resize_token_embeddings(len(base_tok))     # adjust embedding size\n",
    "    # fo one more forward\n",
    "    tmp = base_tok(\"hello [NEW_TAG] world\", return_tensors=\"pt\")\n",
    "    _ = base_model(**tmp)\n",
    "    print(\"[3.2] pretrained + resize embeddings ok; vocab size:\", len(base_tok))\n",
    "except Exception as e:\n",
    "    print(\"[3.2] (skipped) cannot load pretrained model:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006a4ab-d2c7-493c-b87d-4d464f315966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
