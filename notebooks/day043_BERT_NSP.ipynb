{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5349467a-cb76-426c-93b2-392e36dd7efc",
   "metadata": {},
   "source": [
    "# 1. Imports & setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ced36dfb-d610-4059-b2e0-117e07faff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BertForNextSentencePrediction,\n",
    "    BertForPreTraining,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719f120-509a-4600-9853-a3057dd61d25",
   "metadata": {},
   "source": [
    "# 2. Tiny documents & simple sentence splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362d5e6c-4a08-4b2b-ab15-50ecda5e9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    "    \"Transformers are powerful models. They use self attention. BERT is bidirectional. We can pretrain with MLM and NSP.\",\n",
    "    \"Sentence pairs can test coherence. Random pairs are negatives. True next sentences are positives. Sampling matters.\",\n",
    "    \"Pretraining builds contextual embeddings. Fine tuning adapts to tasks. Classification uses CLS. NER uses token labels.\",\n",
    "    \"Tokenization splits text into wordpieces. WordPiece reduces OOV. Special tokens include CLS and SEP. Padding helps batching.\",\n",
    "]\n",
    "\n",
    "def naive_split_sentence(doc):\n",
    "    \"\"\"split based on . and use strip filter empty string\"\"\"\n",
    "    return [s.strip() for s in doc.split(\".\") if s.strip()]\n",
    "\n",
    "docs_sents = [naive_split_sentence(d) for d in DOCS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c395362e-ab9b-4fab-9d0c-5ed30a813ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Transformers are powerful models',\n",
       "  'They use self attention',\n",
       "  'BERT is bidirectional',\n",
       "  'We can pretrain with MLM and NSP'],\n",
       " ['Sentence pairs can test coherence',\n",
       "  'Random pairs are negatives',\n",
       "  'True next sentences are positives',\n",
       "  'Sampling matters'],\n",
       " ['Pretraining builds contextual embeddings',\n",
       "  'Fine tuning adapts to tasks',\n",
       "  'Classification uses CLS',\n",
       "  'NER uses token labels'],\n",
       " ['Tokenization splits text into wordpieces',\n",
       "  'WordPiece reduces OOV',\n",
       "  'Special tokens include CLS and SEP',\n",
       "  'Padding helps batching']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b26ad02-e13a-4da0-aaec-cedba269ca40",
   "metadata": {},
   "source": [
    "# 3. Build NSP pairs (50/50, optional hard negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34b87d72-3053-4e43-8fd7-b6ae035a9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_nsp_pairs(docs, num_pairs=800, pos_ratio=0.5, use_hard_neg=False):\n",
    "    \"\"\"\n",
    "    Return a list od dicts: {\"text_a\": str, \"text_b\":str, \"label\": int}\n",
    "    label 1=IsNext 0=NotNext\n",
    "    pos example: A next(A)\n",
    "    neg example: A random_other_doc_sentence\n",
    "    if use_hard_neg, then part of the neg examples using same doc not neighboring sentences: A not_neighboring_same_doc_sentence\n",
    "    \"\"\"\n",
    "    rng = random.Random(SEED)\n",
    "    all_pairs = []\n",
    "    \n",
    "    #flatten for easy sampling\n",
    "    all_sents = [(d_i, i, s) for d_i, doc in enumerate(docs) for i, s in enumerate(doc)]\n",
    "\n",
    "    while len(all_pairs) < num_pairs:\n",
    "        want_pos = rng.random() < pos_ratio\n",
    "\n",
    "        #sample a doc with at least 2 sentences for positives\n",
    "        d_i = rng.randrange(len(docs))\n",
    "        doc = docs[d_i]\n",
    "        if len(doc) < 2:\n",
    "            continue\n",
    "\n",
    "        if want_pos:\n",
    "            #positive example pick i whre next exists\n",
    "            i = rng.randrange(len(doc) - 1)\n",
    "            a, b = doc[i], doc[i + 1]\n",
    "            all_pairs.append({\"text_a\": a, \"text_b\": b, \"label\": 1})\n",
    "        else:\n",
    "            #negative example\n",
    "            a_idx = rng.randrange(len(doc))\n",
    "            a = doc[a_idx]\n",
    "            if use_hard_neg and len(doc) > 2 and rng.random() < 0.5:\n",
    "                #same doc, but not adgancet\n",
    "                candidates = [j for j in range(len(doc)) if j not in {a_idx-1, a_idx, a_idx+1} and j >= 0 and j < len(doc)]\n",
    "                if not candidates:\n",
    "                    continue\n",
    "                j = rng.choice(candidates)\n",
    "                b = doc[j]\n",
    "            else:\n",
    "                #different doc\n",
    "                other_doc_idx = rng.randrange(len(docs))\n",
    "                while other_doc_idx == d_i:\n",
    "                    other_doc_idx = rng.randrange(len(docs))\n",
    "                other_doc = docs[other_doc_idx]\n",
    "                b = rng.choice(other_doc)\n",
    "            all_pairs.append({\"text_a\": a, \"text_b\": b, \"label\": 0})\n",
    "\n",
    "    rng.shuffle(all_pairs)\n",
    "    return all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c06a4f-1b40-4524-8040-c8f8ed259c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Build] train_pairs=800, val_pairs=200\n"
     ]
    }
   ],
   "source": [
    "# small splits for demo\n",
    "train_pairs = build_nsp_pairs(docs_sents, num_pairs=800, pos_ratio=0.5, use_hard_neg=True)\n",
    "val_pairs   = build_nsp_pairs(docs_sents, num_pairs=200, pos_ratio=0.5, use_hard_neg=True)\n",
    "print(f\"[Build] train_pairs={len(train_pairs)}, val_pairs={len(val_pairs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8dd465-1964-4f83-83ac-6b76f92aa07d",
   "metadata": {},
   "source": [
    "# 4. Datasets yielding raw sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56ff2da6-5346-4edb-8582-cb90fc61f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSPPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item is a dict with raw strings\n",
    "    Collator will tokenize and build tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.pairs[idx]\n",
    "\n",
    "train_ds = NSPPairDataset(train_pairs)\n",
    "val_ds   = NSPPairDataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd8b09-80df-4f38-9254-a27ff6116fe2",
   "metadata": {},
   "source": [
    "# 5. Collator A: NSP-only (no MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9103ce34-c11e-44d8-a114-c11ef251f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NSPOnlyCollator:\n",
    "    \"\"\"\n",
    "    Tokenize sentence pairs for NSP-only training\n",
    "    Output keys:\n",
    "        - input_ids       [B, L]\n",
    "        - attention_mask  [B, L]\n",
    "        - token_type_ids  [B, L]\n",
    "        - next_sentence_label [B] (0/1)\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    max_length: int = 128\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        a_list = [x[\"text_a\"] for x in batch]\n",
    "        b_list = [x[\"text_b\"] for x in batch]\n",
    "        labels = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "        enc = self.tokenizer(\n",
    "            a_list, b_list,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        enc[\"next_sentence_label\"] = labels\n",
    "        return enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94208cb3-f18d-47a0-8b12-4004d7eb8fc0",
   "metadata": {},
   "source": [
    "# 6. Train NSP-only with BertForNextSentencePredition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c26d3353-5998-478d-a56c-40faebd59f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/bcrb9sq920z3c4x2gm1wy70h0000gn/T/ipykernel_75679/3818745471.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_nsp = Trainer(\n",
      "/Users/mamba/Documents/MOOC/ML/env/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1396: FutureWarning: The `next_sentence_label` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train-NSP] Starting …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:13, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Nsp Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>0.019237</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval-NSP] Final: {'eval_loss': 0.0001811510737752542, 'eval_nsp_acc': 1.0, 'eval_runtime': 0.3052, 'eval_samples_per_second': 655.267, 'eval_steps_per_second': 42.592, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "collator_nsp = NSPOnlyCollator(tokenizer=tokenizer, max_length=64)\n",
    "\n",
    "model_nsp = BertForNextSentencePrediction.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "\n",
    "args_nsp = TrainingArguments(\n",
    "    output_dir=\"out_nsp\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"next_sentence_label\"]\n",
    ")\n",
    "\n",
    "def nsp_accuracy(eval_pred):\n",
    "    \"\"\"\n",
    "    predictions: [N, 2]\n",
    "    labels_ids: [N]\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "    if isinstance(preds, (tuple, list)):\n",
    "        preds = preds[0]\n",
    "    pred = torch.from_numpy(preds).argmax(-1)\n",
    "    labels = torch.from_numpy(labels)\n",
    "    acc = (pred == labels).float().mean().item()\n",
    "    return {\"nsp_acc\": acc}\n",
    "\n",
    "trainer_nsp = Trainer(\n",
    "    model=model_nsp,\n",
    "    args=args_nsp,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator_nsp,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=nsp_accuracy,\n",
    ")\n",
    "\n",
    "print(\"\\n[Train-NSP] Starting …\")\n",
    "trainer_nsp.train()\n",
    "print(\"[Eval-NSP] Final:\", trainer_nsp.evaluate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288355bb-6b8b-4946-a861-1aa8a0023835",
   "metadata": {},
   "source": [
    "# 7. Collator B: MLM + NSP (dynamic MLM 15% + 80/10/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7f1c6431-0013-4af0-9123-6ebd6a0ac8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PretrainCollator:\n",
    "    \"\"\"\n",
    "    Build inputs for MLM + NSP:\n",
    "        - Tokenize pair (A, B) with BERT template\n",
    "        - Dynamic MLM (15% + 80/10/10) to produce \"labels\" (MLM) and masked input_ids\n",
    "        - Provide \"next_sentence_label\" (0/1) for NSP\n",
    "    Outputs:\n",
    "        inputs_ids [B, L] (masked)\n",
    "        attention_mask [B, L]\n",
    "        token_type_ids [B, L]\n",
    "        labels [B, L] (MLM: -100 for non-masked)\n",
    "        next_sentence_label [B]\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    mlm_probability: float = 0.15\n",
    "    max_length: int = 128\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        a_list = [x[\"text_a\"] for x in batch]\n",
    "        b_list = [x[\"text_b\"] for x in batch]\n",
    "        nsp = torch.tensor([x[\"label\"] for x in batch], dtype=torch.long)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            a_list,\n",
    "            b_list,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"]                     #[B, L]\n",
    "        attention_mask = enc[\"attention_mask\"]           #[B, L]\n",
    "        special_mask = enc[\"special_tokens_mask\"].bool()  #[B, L]\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)\n",
    "        candidate = (~special_mask) & attention_mask.bool()\n",
    "\n",
    "        #Use Bernuolli (can change to WWM/Span)\n",
    "        probs = torch.full_like(input_ids, self.mlm_probability, dtype=torch.float32)\n",
    "        chosen = (torch.bernoulli(probs).bool()) & candidate\n",
    "\n",
    "        labels[chosen] = input_ids[chosen]\n",
    "\n",
    "        r = torch.rand_like(input_ids, dtype=torch.float32)\n",
    "        replace_mask = chosen & (r < 0.8)\n",
    "        replace_rand = chosen & (r >= 0.8) & (r < 0.9)\n",
    "\n",
    "        masked = input_ids.clone()\n",
    "        masked[replace_mask] = self.tokenizer.mask_token_id\n",
    "\n",
    "        if replace_rand.any():\n",
    "            vocab_size = self.tokenizer.vocab_size\n",
    "            rand_ids = torch.randint(low=0, high=vocab_size, size=masked.shape, dtype=torch.long)\n",
    "            specials = set(self.tokenizer.all_special_ids)\n",
    "            bad = torch.isin(rand_ids, torch.tensor(list(specials)))\n",
    "            if bad.any():\n",
    "                rand_ids[bad] = self.tokenizer.unk_token_id\n",
    "            masked[replace_rand] = rand_ids[replace_rand]\n",
    "\n",
    "        enc_out = {\n",
    "            \"input_ids\": masked,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"next_sentence_label\": nsp\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in enc:\n",
    "            enc_out[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a509f5e-c556-467d-9ef7-d4aca8ccd1f0",
   "metadata": {},
   "source": [
    "# 8. Train MLM + NSP with BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74f05933-d39f-494f-8861-40c7c66cee44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/bcrb9sq920z3c4x2gm1wy70h0000gn/T/ipykernel_75679/739160025.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_pre = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train-Pretraining] Starting …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Masked Acc</th>\n",
       "      <th>Nsp Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.293800</td>\n",
       "      <td>0.710997</td>\n",
       "      <td>0.908847</td>\n",
       "      <td>0.965000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583700</td>\n",
       "      <td>0.355018</td>\n",
       "      <td>0.948229</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval-Pretraining] Final: {'eval_loss': 0.2849549651145935, 'eval_masked_acc': 0.9342465753424658, 'eval_nsp_acc': 0.99, 'eval_runtime': 3.3367, 'eval_samples_per_second': 59.939, 'eval_steps_per_second': 3.896, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "collator_pre = PretrainCollator(tokenizer=tokenizer, mlm_probability=0.15, max_length=64)\n",
    "model_pre = BertForPreTraining.from_pretrained(\"bert-base-uncased\").to(DEVICE)\n",
    "\n",
    "args_pre = TrainingArguments(\n",
    "    output_dir=\"out_pretrain\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,            # demo\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_ratio=0.1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=[],\n",
    "    seed=SEED,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    "    # return both labels\n",
    "    label_names=[\"labels\", \"next_sentence_label\"],\n",
    ")\n",
    "\n",
    "\n",
    "def metrics_pretrain(eval_pred):\n",
    "    \"\"\"\n",
    "    For BertForPreTraining:\n",
    "      predictions is a tuple: (prediction_scores [N,L,V], seq_relationship_logits [N,2])\n",
    "      label_ids corresponds to label_names: (mlm_labels [N,L], nsp_labels [N])\n",
    "    We'll compute:\n",
    "      - masked-token accuracy (on MLM)\n",
    "      - NSP accuracy\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "    # preds\n",
    "    if isinstance(preds, (tuple, list)) and len(preds) >= 2:\n",
    "        mlm_logits, nsp_logits = preds[0], preds[1]\n",
    "    else:\n",
    "        return {\"masked_acc\": 0.0, \"nsp_acc\": 0.0}\n",
    "\n",
    "    # labels\n",
    "    if isinstance(labels, (tuple, list)) and len(labels) >= 2:\n",
    "        mlm_labels, nsp_labels = labels[0], labels[1]\n",
    "    else:\n",
    "        mlm_labels, nsp_labels = labels, None\n",
    "\n",
    "    # MLM masked-token accuracy\n",
    "    mlm_pred = mlm_logits.argmax(-1)               # [N,L]\n",
    "    mask = (mlm_labels != -100)\n",
    "    mt_correct = ((mlm_pred == mlm_labels) & mask).sum()\n",
    "    mt_total = mask.sum().clip(min=1)\n",
    "    masked_acc = (mt_correct / mt_total).item()\n",
    "\n",
    "    # NSP accuracy\n",
    "    if nsp_labels is not None:\n",
    "        nsp_pred = nsp_logits.argmax(-1)\n",
    "        nsp_acc = (nsp_pred == nsp_labels).mean().item()\n",
    "    else:\n",
    "        nsp_acc = 0.0\n",
    "\n",
    "    return {\"masked_acc\": masked_acc, \"nsp_acc\": nsp_acc}\n",
    "\n",
    "trainer_pre = Trainer(\n",
    "    model=model_pre,\n",
    "    args=args_pre,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator_pre,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=metrics_pretrain,\n",
    ")\n",
    "\n",
    "print(\"\\n[Train-Pretraining] Starting …\")\n",
    "trainer_pre.train()\n",
    "print(\"[Eval-Pretraining] Final:\", trainer_pre.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6582721-bbc8-4804-a496-f309dc9ae6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
