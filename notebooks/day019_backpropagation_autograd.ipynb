{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e478801c-a08a-4e91-b29a-82abcea9452a",
   "metadata": {},
   "source": [
    "# 1. Backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c921391-5e46-4e6f-a1ca-9cd1d46afde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b166e797-df45-46d1-9a37-43007aaf9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1., 2.]])\n",
    "w = np.array([[0.5], [1.0]])\n",
    "b = 0.1\n",
    "y = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf95644d-949d-4e5c-af8a-56f22fcde68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X @ w + b\n",
    "loss = (a - y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d93eff0-0152-4ce8-9f3a-7f40ab24f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dL_da = 2 * (a - y)\n",
    "da_dw = X.T\n",
    "da_dX = w.T\n",
    "da_db = 1\n",
    "dL_dw = dL_da * da_dw\n",
    "dL_db = dL_da * da_db\n",
    "dL_dX = dL_da * da_dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fdc0c2c-1cf1-4380-9474-00a851c4dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dw: [[3.2]\n",
      " [6.4]]\n",
      "dL_dX: [[1.6 3.2]]\n",
      "dL_db: [[3.2]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"dL_dw: {dL_dw}\")\n",
    "print(f\"dL_dX: {dL_dX}\")\n",
    "print(f\"dL_db: {dL_db}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371e1eaf-5d6d-41d3-a55a-4247f6f396ce",
   "metadata": {},
   "source": [
    "# 2. MLP manually backprop \n",
    "day18 notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604d9c5-1e70-41db-bcb5-0f4afee3b047",
   "metadata": {},
   "source": [
    "# 3. PyTorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89dec289-0a1e-4693-952e-12ab4dfcd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61f80c41-fc5a-4caf-857a-980aef34dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
    "w = torch.tensor([[0.5], [1.0]], requires_grad=True)\n",
    "b = torch.tensor([0.1], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61caa58d-8546-4bf7-ba99-91dc9ad22520",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X @ w + b\n",
    "loss = (y_pred - 1.0) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5101065-2b31-47ea-8528-d55b2823cdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9a03c64-0d3c-4e15-a601-5a3827f01c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss grad: None\n",
      "y_pred grad: None\n",
      "w grad: tensor([[3.2000],\n",
      "        [6.4000]])\n",
      "X grad: tensor([[1.6000, 3.2000]])\n",
      "b grad: tensor([3.2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/bcrb9sq920z3c4x2gm1wy70h0000gn/T/ipykernel_79069/3063013896.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"loss grad: {loss.grad}\")\n",
      "/var/folders/ky/bcrb9sq920z3c4x2gm1wy70h0000gn/T/ipykernel_79069/3063013896.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"y_pred grad: {y_pred.grad}\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"loss grad: {loss.grad}\")\n",
    "print(f\"y_pred grad: {y_pred.grad}\")\n",
    "print(f\"w grad: {w.grad}\")\n",
    "print(f\"X grad: {X.grad}\")\n",
    "print(f\"b grad: {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "260cd6bd-91b0-40f1-8c12-fd9b5cb243cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detach\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2\n",
    "z = y.detach()  # ✅ 正确：z 是张量，且不再参与梯度追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1705144-a3b1-4341-9730-d60b26f3afbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "# no_grad 示例\n",
    "with torch.no_grad():\n",
    "    print(x * 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a95bb8-a677-43e2-844a-e4bba255beb0",
   "metadata": {},
   "source": [
    "# 4. Tensorflow GradientTape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeeff73f-15e4-4cae-85fb-18412498fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd502b9e-7328-4757-9e8c-08afe5ecec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.Variable([[1.0, 2.0]])\n",
    "w = tf.Variable([[0.5], [1.0]])\n",
    "b = tf.Variable([0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b49aba40-230f-4f47-96e2-f2cb3bbd0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    y_pred = tf.matmul(X, w) + b\n",
    "    loss = (y_pred - 1.0) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5ed0dcea-e796-4f26-a812-5f7987a6caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = tape.gradient(loss, [w, X, b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e973a044-2fa3-481b-9847-d40a1ac44441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dw: [[3.1999998]\n",
      " [6.3999996]]\n",
      "dX: [[1.5999999 3.1999998]]\n",
      "db: [3.1999998]\n"
     ]
    }
   ],
   "source": [
    "print(\"dw:\", grads[0].numpy())  # ∂loss/∂w\n",
    "print(\"dX:\", grads[1].numpy())  # ∂loss/∂X\n",
    "print(\"db:\", grads[2].numpy())  # ∂loss/∂b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7502b2a-3d7c-4eda-9460-2c1e0def8857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
