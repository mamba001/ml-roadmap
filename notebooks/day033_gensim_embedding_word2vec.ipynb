{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14eae6fd-376e-4256-b073-4dd0aa1f7fe5",
   "metadata": {},
   "source": [
    "# 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9497fe99-e82e-41fa-b69e-2ddf2dbaea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5561929c-b693-41fe-a9e6-6045b1aea465",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"this movie is great and the acting is excellent\",\n",
    "    \"what a fantastic film with wonderful direction\",\n",
    "    \"the plot is good and the soundtrack is amazing\",\n",
    "    \"the story is touching and performances are strong\",\n",
    "    \"a brilliant and engaging narrative overall\",\n",
    "\n",
    "    \"this movie is bad and the pacing is awful\",\n",
    "    \"the film is boring with dull characters\",\n",
    "    \"terrible editing and horrible dialogue\",\n",
    "    \"a predictable script and poor scenes\",\n",
    "    \"unwatchable messy scenes and weak plot\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee69e470-5adf-456f-a4d0-91980477a883",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([1,1,1,1,1,0,0,0,0,0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f367a750-1efd-4b0d-a486-c5dd9aa0964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    return re.findall(r\"[a-z]+\", s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "225b0bc8-ba30-4c3a-aacf-4ed7c96423fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Example tokenized sentence: ['this', 'movie', 'is', 'great', 'and', 'the', 'acting', 'is', 'excellent']\n"
     ]
    }
   ],
   "source": [
    "sentences = [tokenize(line) for line in corpus]\n",
    "print(\"[Info] Example tokenized sentence:\", sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd63dd-a4fb-485c-840d-a79c2f067709",
   "metadata": {},
   "source": [
    "# 1. Numpy: PPMI + SVD for word vectors (Positive Pointwise Mutual Information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468c358b-5dfc-48ec-9dd5-2cfc8615277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab_from_sentences(sentences, min_count=1):\n",
    "    freq = Counter(w for sent in sentences for w in sent)\n",
    "    tokens = [w for w, c in freq.items() if c >= min_count]\n",
    "    tokens.sort()\n",
    "    vocab = {w : i for i, w in enumerate(tokens)}\n",
    "    ivocab = {i : w for w, i in vocab.items()}\n",
    "    return vocab, ivocab, freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb5b41fc-e583-4be4-8682-c9277d916f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooc_matrix(sentences, vocab, window=5):\n",
    "    \"\"\"\n",
    "    build co-occurrence matrix, which shape is (V, V) where C[i, j] is word i and word j co-occurence number of times in window\n",
    "    \"\"\"\n",
    "    V = len(vocab)\n",
    "    C = np.zeros((V, V), dtype=np.float64)\n",
    "    for sent in sentences:\n",
    "        idxs = [vocab[w] for w in sent if w in vocab]\n",
    "        for i, ci in enumerate(idxs):\n",
    "            #can use random window, here use fixed window\n",
    "            left = max(0, i - window)\n",
    "            right = min(len(idxs), i + window + 1)\n",
    "            for j in range(left, right):\n",
    "                if j == i:\n",
    "                    continue\n",
    "                cj = idxs[j]\n",
    "                C[ci, cj] += 1.0\n",
    "\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49200793-75c0-4751-8b3e-862ab29f7ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppmi(C):\n",
    "    \"\"\" PPMI_{ij} = max(0, log( (C_ij * N) / (C_i* * C_*j) )) \"\"\"\n",
    "    total = C.sum()\n",
    "    row = C.sum(1, keepdims=True)\n",
    "    col = C.sum(0, keepdims=True)\n",
    "\n",
    "    denom = (row @ col)\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        PMI = np.log((C * total + 1e-12) / (denom + 1e-12))\n",
    "    PPMI = np.maximum(PMI, 0.0)\n",
    "    return PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "375dc447-041d-466a-a651-f2a8bbbf9b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_embeddings(PPMI, dim=100, use_sqrt=True):\n",
    "    \"\"\"\n",
    "    do SVD to PPMI: PPMI = U S V.T\n",
    "    take U * S^{1/2} as word vectors\n",
    "\n",
    "    PPMI: (V, V)\n",
    "    U: (V, V)\n",
    "    S: (V, )\n",
    "    Vt: (V, V)\n",
    "    W: (V, dim)\n",
    "    \"\"\"\n",
    "    U, S, Vt = np.linalg.svd(PPMI, full_matrices=False)\n",
    "    if dim < U.shape[1]:\n",
    "        U, S = U[:, :dim], S[:dim]\n",
    "    if use_sqrt:\n",
    "        W = U * np.sqrt(S)[None, :]\n",
    "    else:\n",
    "        W = U * S[None, :]\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18c1e25-8876-4d6d-9709-1916c91df763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b, eps=1e-9):\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) + eps) / (np.linalg.norm(b) + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b121622-f819-450e-96df-621106c4a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_from_matrix(W, ivocab, query, vocab, topn=5):\n",
    "    if query not in vocab:\n",
    "        return f\"{query} OOV\"\n",
    "    qv = W[vocab[query]]\n",
    "    sims = (W @ qv) / (np.linalg.norm(W, axis=1) * np.linalg.norm(qv) + 1e-9)\n",
    "    order = np.argsort(-sims)\n",
    "    out = []\n",
    "    for i in order:\n",
    "        w = ivocab[i]\n",
    "        if w == query:\n",
    "            continue\n",
    "        out.append((w, float(sims[i])))\n",
    "        if len(out) >= topn:\n",
    "            break\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5c1b053-e403-4db0-a8bd-354d8da2051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, ivocab, freq = build_vocab_from_sentences(sentences, min_count=1)\n",
    "C = build_cooc_matrix(sentences, vocab, window=5)\n",
    "PPMI = compute_ppmi(C)\n",
    "W_ppmi = svd_embeddings(PPMI, dim=100, use_sqrt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b20eb1cb-30fa-47c5-8910-fab538d13241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPMI+SVD] Vocab=45, Embedding shape=(45, 45)\n",
      "[PPMI+SVD] most similar to 'great': [('acting', 0.34596382230724165), ('the', 0.30619822201217717), ('movie', 0.25211923185218205), ('and', 0.2197068643204385), ('is', 0.20462559347433978)]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[PPMI+SVD] Vocab={len(vocab)}, Embedding shape={W_ppmi.shape}\")\n",
    "print(\"[PPMI+SVD] most similar to 'great':\", most_similar_from_matrix(W_ppmi, ivocab, \"great\", vocab, topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a882d-abe3-46a3-a09e-b6ddf9531bd1",
   "metadata": {},
   "source": [
    "# 2. Gensim: Word2Vec (CBOW/Skip-gram + Negative Sampling/HS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a87c2f34-5a97-43be-a669-3aa183b47268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gensim] Vocab size = 45\n",
      "[Gensim] most similar to 'excellent': [('predictable', 0.14087845385074615), ('wonderful', 0.11117531359195709), ('a', 0.1084885522723198), ('messy', 0.08125098049640656), ('script', 0.08116475492715836)]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    w2v = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        sg=1, # 1=skip-gram, 0=CBOW\n",
    "        negative=5, #number of negative sampling\n",
    "        hs=0, #hierarchical sofmaxt \n",
    "        sample=1e-3,\n",
    "        workers=2,\n",
    "        epochs=10,\n",
    "        seed=42\n",
    "    )\n",
    "    wv = w2v.wv #KeyedVectors：contians token->vector\n",
    "    print(\"[Gensim] Vocab size =\", len(wv))\n",
    "    if \"excellent\" in wv:\n",
    "        print(\"[Gensim] most similar to 'excellent':\", wv.most_similar(\"excellent\", topn=5))\n",
    "except Exception as e:\n",
    "    print(\"[Gensim] Not installed or error:\", repr(e))\n",
    "    w2v, wv = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a85f1fb-7d8f-4a0a-af98-c4331ca864f5",
   "metadata": {},
   "source": [
    "# 3. DocEmb = TF-IDF x Embedding + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70f79d22-0696-4337-9c03-912473b86670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_emb_by_tfidf_times_embedding(texts, labels, embed_tokens, embed_matrix, topk_sim_demo=True):\n",
    "    \"\"\"\n",
    "    texts: List[str]\n",
    "    embed_tokens: List[str] 与 embed_matrix 的行严格对齐\n",
    "    embed_matrix: np.ndarray, shape [V_e, d]\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.svm import LinearSVC\n",
    "        from sklearn.metrics import accuracy_score\n",
    "\n",
    "        #1) calculate TF-IDF\n",
    "        tfv = TfidfVectorizer(lowercase=True, token_pattern=r\"[A-Za-z]+\", stop_words=\"english\", ngram_range=(1,1), sublinear_tf=True)\n",
    "        X = tfv.fit_transform(texts) #[N, V_tfv]\n",
    "        vocab_tfv = tfv.vocabulary_ #str -> col index\n",
    "\n",
    "        #2) match vocab: only words that common\n",
    "        embed_index = {w:i for i, w in enumerate(embed_tokens)}\n",
    "        common = sorted(set(vocab_tfv.keys()) & set(embed_index.keys()))\n",
    "        if len(common) == 0:\n",
    "            print(\"[DocEmb] No common tokens between TF-IDF vocab and embedding vocab.\")\n",
    "            return\n",
    "\n",
    "        cols = np.array([vocab_tfv[w] for w in common], dtype=int)\n",
    "        rows = np.array([embed_index[w] for w in common], dtype=int)\n",
    "        X_sub = X[:, cols] #[N, |C|]\n",
    "        W_sub = embed_matrix[rows, :]  #[|C|, d]\n",
    "\n",
    "        #3). DocEmb = TF-IDF x Embedding\n",
    "        DocEmb = X_sub @ W_sub #[N,d]\n",
    "        DocEmb = np.asarray(DocEmb)\n",
    "\n",
    "        #4). train and eval\n",
    "        #TF-IDF x Embedding\n",
    "        clf_lr = LogisticRegression(max_iter=1000)\n",
    "        clf_lr.fit(DocEmb, labels)\n",
    "        pred_lr = clf_lr.predict(DocEmb)\n",
    "        acc_lr = accuracy_score(labels, pred_lr)\n",
    "\n",
    "        # only TF-IDF \n",
    "        clf_lr2 = LogisticRegression(max_iter=1000)\n",
    "        clf_lr2.fit(X, labels)\n",
    "        pred_lr2 = clf_lr2.predict(X)\n",
    "        acc_lr2 = accuracy_score(labels, pred_lr2)\n",
    "\n",
    "\n",
    "        print(f\"[DocEmb] LogisticRegression on DocEmb(TF-IDF×Embedding) acc={acc_lr:.3f}  vs  pure TF-IDF acc={acc_lr2:.3f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"[DocEmb] Need scikit-learn installed:\", repr(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183f2539-16af-4991-bf0b-5bcad2e04e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DocEmb] LogisticRegression on DocEmb(TF-IDF×Embedding) acc=1.000  vs  pure TF-IDF acc=1.000\n",
      "[DocEmb] LogisticRegression on DocEmb(TF-IDF×Embedding) acc=1.000  vs  pure TF-IDF acc=1.000\n"
     ]
    }
   ],
   "source": [
    "# —— PPMI+SVD -> DocEmb ——\n",
    "embed_tokens_ppmi = [ivocab[i] for i in range(len(ivocab))]\n",
    "embed_matrix_ppmi = W_ppmi\n",
    "doc_emb_by_tfidf_times_embedding(corpus, labels, embed_tokens_ppmi, embed_matrix_ppmi)\n",
    "\n",
    "# —— Gensim Word2Vec -> DocEmb ——\n",
    "if wv is not None:\n",
    "    embed_tokens_w2v = list(wv.key_to_index.keys())    # 与 wv.vectors 行严格对齐\n",
    "    embed_matrix_w2v = wv.vectors\n",
    "    doc_emb_by_tfidf_times_embedding(corpus, labels, embed_tokens_w2v, embed_matrix_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ed1aa-4541-44f6-a847-bac3ad9bff7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
