{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf7b62d8-8665-4973-95c5-2bc4c1892a43",
   "metadata": {},
   "source": [
    "# 1. setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7680e9c0-b7c1-4f0c-909c-b1cab80673d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os, random\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d73716a-6560-4713-bcf2-5df73acc993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, BertForMaskedLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e65eea13-d676-491f-9a40-6d6a6d7a917b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Device: mps\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"mps\")\n",
    "print(\"[1] Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08890805-8ec7-41de-8780-8963e86b9634",
   "metadata": {},
   "source": [
    "# 2. Tiny toy corpus and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990880ec-4b9e-465f-85bb-ec71c21ef8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS = [\n",
    "    \"Transformers encode rich bidirectional context.\",\n",
    "    \"Masked language modeling enables bidirectional learning.\",\n",
    "    \"BERT uses WordPiece tokenization and special tokens.\",\n",
    "    \"We randomly mask about fifteen percent of tokens.\",\n",
    "    \"Sometimes we keep original words to avoid overfitting to [MASK].\",\n",
    "    \"Occasionally a random token is inserted to add noise.\",\n",
    "    \"This training objective builds strong contextual embeddings.\",\n",
    "    \"Whole Word Masking groups wordpieces of the same word together.\",\n",
    "    \"Span masking masks consecutive tokens to model phrases.\",\n",
    "    \"Pretraining can later be fine tuned for downstream tasks.\"\n",
    "] * 50  # ~500 lines for a quick demo\n",
    "\n",
    "split = int(len(CORPUS) * 0.9)\n",
    "train_texts = CORPUS[:split]\n",
    "val_texts = CORPUS[split:]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns raw strings; custom data_collator will tokenize & create MLM labels.\n",
    "    Output of __getitem__(i): str\n",
    "    \"\"\"\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f03f431-6985-4133-b189-7826f318c464",
   "metadata": {},
   "source": [
    "# 3. Manual MLM collator (15% + 80/10/10, supports WWM/Span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc07adf9-4e28-4edb-8b10-c76929045c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLMCollatorManual:\n",
    "    \"\"\"\n",
    "    Build an MLM batch with dynamic masking.\n",
    "    Outputs dict of tensors:\n",
    "      - input_ids      [B, L] (masked inputs)\n",
    "      - attention_mask [B, L]\n",
    "      - token_type_ids [B, L] (if tokenizer provides)\n",
    "      - labels         [B, L] (non-masked positions = -100)\n",
    "    \"\"\"\n",
    "    tokenizer: any\n",
    "    mlm_probability: float = 0.15\n",
    "    max_length: int = 128\n",
    "    whole_word_mask: bool = False\n",
    "    span_mask: bool = False\n",
    "    mean_span_len: float = 3.0\n",
    "\n",
    "    def __call__(self, batch_texts):\n",
    "        # 1) Tokenize (dynamic padding up to batch longest, capped by max_length)\n",
    "        enc = self.tokenizer(\n",
    "            batch_texts,\n",
    "            padding=\"longest\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]                     # [B, L]\n",
    "        attention_mask = enc[\"attention_mask\"]           # [B, L]\n",
    "        special_mask = enc[\"special_tokens_mask\"].bool() # [B, L] such as CLS SEP\n",
    "\n",
    "        labels = torch.full_like(input_ids, -100)        # [B, L], -100 ignored by CE\n",
    "        candidate = (~special_mask) & attention_mask.bool() # not special and need attention's\n",
    "\n",
    "        # 2) Choose masked positions\n",
    "        if self.whole_word_mask:\n",
    "            chosen = self._choose_wwm(enc, candidate, self.mlm_probability)  # [B, L] bool\n",
    "        elif self.span_mask:\n",
    "            chosen = self._choose_spans(candidate, self.mlm_probability, self.mean_span_len)\n",
    "        else:\n",
    "            probs = torch.full_like(input_ids, self.mlm_probability, dtype=torch.float32)\n",
    "            chosen = (torch.bernoulli(probs).bool()) & candidate\n",
    "\n",
    "        # 3) 80/10/10 replacement\n",
    "        labels[chosen] = input_ids[chosen] #only masked positions contribute to loss, other places are all -100, maksed position are input token ids\n",
    "        r = torch.rand_like(input_ids, dtype=torch.float32) #uniform distribution on interval [0,1) of size input_ids shape\n",
    "        replace_mask = chosen & (r < 0.8)              # -> [MASK]\n",
    "        replace_rand = chosen & (r >= 0.8) & (r < 0.9) # -> random token\n",
    "        #remaining 10% keep original\n",
    "\n",
    "        masked = input_ids.clone()\n",
    "        masked[replace_mask] = self.tokenizer.mask_token_id\n",
    "\n",
    "        if replace_rand.any():\n",
    "            vocab_size = self.tokenizer.vocab_size\n",
    "            rand_ids = torch.randint(low=0, high=vocab_size, size=masked.shape, dtype=torch.long)\n",
    "            specials = set(self.tokenizer.all_special_ids)\n",
    "            bad = torch.isin(rand_ids, torch.tensor(list(specials)))\n",
    "            if bad.any():\n",
    "                rand_ids[bad] = self.tokenizer.unk_token_id\n",
    "\n",
    "            masked[replace_rand] = rand_ids[replace_rand]\n",
    "\n",
    "        out = {\n",
    "            \"input_ids\": masked,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "        if \"token_type_ids\" in enc:\n",
    "            out[\"token_type_ids\"] = enc[\"token_type_ids\"]\n",
    "        return out\n",
    "\n",
    "    def _choose_wwm(self, enc, candidate, p):\n",
    "        \"\"\"whole word masking via word_ids grouping\"\"\"\n",
    "        B, L = candidate.shape\n",
    "        chosen = torch.zeros_like(candidate, dtype=torch.bool)\n",
    "        for i in range(B):\n",
    "            word_ids = enc.word_ids(batch_index=i) #len L (ints or None)\n",
    "            groups = {}\n",
    "            for t, wid in enumerate(word_ids):\n",
    "                if wid is None:\n",
    "                    continue\n",
    "                if not candidate[i, t]:\n",
    "                    continue\n",
    "                groups.setdefault(wid, []).append(t)\n",
    "\n",
    "            group_list = list(groups.values())\n",
    "            random.shuffle(group_list)\n",
    "            target = int(p * candidate[i].sum().item())\n",
    "            covered = 0\n",
    "            for g in group_list:\n",
    "                for idx in g:\n",
    "                    chosen[i, idx] = True\n",
    "                covered += len(g)\n",
    "                if covered >= target:\n",
    "                    break\n",
    "        return chosen\n",
    "\n",
    "    def _choose_spans(self, canditate, p, mean_span_len=3.0):\n",
    "        \"\"\"span masking using a geometric span length\"\"\"\n",
    "        B, L = candidate.shape\n",
    "        chosen = torch.zeros_like(candidate, dtype=torch.bool)\n",
    "        q = 1.0 / (1.0 + mean_span_len) #geometric param\n",
    "\n",
    "        for i in range(B):\n",
    "            cand_idx = candidate[i].nonzero(as_tuple=False).flatten().tolist()\n",
    "            random.shuffle(cand_idx)\n",
    "            target = int(p * len(cand_idx))\n",
    "            covered, used = 0, set()\n",
    "            while covered < target and cand_idx:\n",
    "                start = random.choice(cand_idx)\n",
    "                if start in used:\n",
    "                    cand_idx.remove(start)\n",
    "                    continue\n",
    "                Ls = 1\n",
    "                while random.random() > q:\n",
    "                    Ls += 1\n",
    "\n",
    "                span = []\n",
    "                t = start\n",
    "                while len(span) < Ls and t < L and candidate[i, t] and (t not in used):\n",
    "                    span.append(t)\n",
    "                    t += 1\n",
    "\n",
    "                for s in span:\n",
    "                    chosen[i, s] = True\n",
    "                    used.add(s)\n",
    "                covered += len(span)\n",
    "        return chosen\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e29b0b-ec9f-4382-a7a0-eb10ae3b4ff1",
   "metadata": {},
   "source": [
    "# 4. Tokenizer and datasets and collator init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6877b40-f850-4ef6-ad88-f5576044a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "train_ds = TextDataset(train_texts)\n",
    "val_ds = TextDataset(val_texts)\n",
    "\n",
    "collator = MLMCollatorManual(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.15,\n",
    "    max_length=64,\n",
    "    whole_word_mask=True,\n",
    "    span_mask=False,\n",
    "    mean_span_len=3.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f23d5e-c899-4f79-ba48-5c86ba9f3e33",
   "metadata": {},
   "source": [
    "# 5. Model (BertForMaskedLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0f6b5ee-08d1-4cb8-8e93-a4f2ec6de5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b49f0e-86e5-4ba7-9180-2735f88d815b",
   "metadata": {},
   "source": [
    "# 6. Training Arguments and compute_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ad82cc-5fd9-4b72-8727-5545dd8a82f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"out\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",     # requested\n",
    "    warmup_ratio=0.1,        # requested\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",      # keep demo light; change to \"epoch\" to save checkpoints\n",
    "    report_to=[],            # disable WandB/Comet by default\n",
    "    seed=SEED,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,  # we pass raw strings; collator handles tokenization\n",
    ")\n",
    "\n",
    "def masked_token_accuracy(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute masked-token accuracy:\n",
    "        predictions: [N, L, V]\n",
    "        labels: [N, L] (non-masked=-100)\n",
    "    \"\"\"\n",
    "    if hasattr(eval_pred, \"predictions\"):\n",
    "        preds = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "    else:\n",
    "        preds, labels = eval_pred\n",
    "\n",
    "    pred_ids = torch.from_numpy(preds).argmax(-1) if not torch.is_tensor(preds) else preds.argmax(-1)\n",
    "    labels = torch.from_numpy(labels) if not torch.is_tensor(labels) else labels\n",
    "    mask = labels.ne(-100)\n",
    "    if mask.sum().item() == 0:\n",
    "        return {\"masked_acc\": 0.0}\n",
    "    correct = (pred_ids.eq(labels) & mask).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return {\"masked_acc\": correct / total}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb5c35-9ece-4600-a72f-f51cc03882b5",
   "metadata": {},
   "source": [
    "# 7. Trainer/Train/Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3150ff61-167d-4290-b7ad-2af18484fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ky/bcrb9sq920z3c4x2gm1wy70h0000gn/T/ipykernel_43616/3859744907.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Starting Trainer.fit() …\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [87/87 00:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Masked Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956133</td>\n",
       "      <td>0.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.252300</td>\n",
       "      <td>0.300960</td>\n",
       "      <td>0.887324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.252300</td>\n",
       "      <td>0.300889</td>\n",
       "      <td>0.940299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Eval] Final: {'eval_loss': 0.29481780529022217, 'eval_masked_acc': 0.9661016949152542, 'eval_runtime': 0.1309, 'eval_samples_per_second': 381.916, 'eval_steps_per_second': 30.553, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,   # manual dynamic masking (15% + 80/10/10)\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=masked_token_accuracy,\n",
    ")\n",
    "\n",
    "print(\"\\n[Train] Starting Trainer.fit() …\")\n",
    "trainer.train()\n",
    "print(\"[Eval] Final:\", trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ae34b-1a4e-4fbd-85fc-90f22bf52a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
