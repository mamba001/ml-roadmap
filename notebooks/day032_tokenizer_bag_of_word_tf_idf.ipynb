{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8367d3d1-58fe-401b-94ce-1b1098715e23",
   "metadata": {},
   "source": [
    "# 0. toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a51975-d83b-4529-8c68-5e6eda060edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "588d81dd-c6dc-41bf-8e33-f11a8067a833",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I absolutely love this movie, the acting is great and the story is touching.\",\n",
    "    \"Terrible plot and boring characters, I regret watching this film.\",\n",
    "    \"What a fantastic experience! Beautiful soundtrack and strong performances.\",\n",
    "    \"This is the worst movie ever; bad editing and a predictable script.\",\n",
    "    \"Wonderful direction and engaging narrative. I would recommend it to everyone.\",\n",
    "    \"Awful pacing. The film drags on and I nearly fell asleep.\",\n",
    "    \"Heartwarming and inspiring â€“ a must watch.\", \n",
    "    \"Unwatchable. Poor dialogue and messy scenes.\"\n",
    "]\n",
    "labels = np.array([1, 0, 1, 0, 1, 0, 1, 0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8326b52a-97db-4aba-b936-d55e249c99ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(texts))\n",
    "rng.shuffle(idx)\n",
    "split = int(0.7 * len(texts))\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "#texts_train = texts[train_idx]  TypeError: only integer scalar arrays can be converted to a scalar index\n",
    "texts_train = [texts[i] for i in train_idx]\n",
    "y_train = labels[train_idx]\n",
    "texts_test = [texts[i] for i in test_idx]\n",
    "y_test = labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94dabc2e-c53a-4d75-81c9-153d614bac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] Train=5, Test=3\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Data] Train={len(texts_train)}, Test={len(texts_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5448127c-cbeb-4586-b3ae-db8ba96a3779",
   "metadata": {},
   "source": [
    "# 1. Numpy: Tokenizer -> vocab -> BOW -> TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74aff84b-3d28-4f3c-9f79-7d9af3607ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenize(s, lower=True):\n",
    "    \"\"\"\n",
    "    simple regex tokenizer (letters only)\n",
    "    \"\"\"\n",
    "    if lower:\n",
    "        s = s.lower()\n",
    "    tokens = re.findall(r\"[a-z]+\", s)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6879ceb6-2037-495d-a059-c8deeb26a87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(docs, min_df=1, max_vocab_size=None):\n",
    "    \"\"\"\n",
    "    build vocabulary: {token -> index}\n",
    "    \"\"\"\n",
    "    df_counter = Counter()\n",
    "    for doc in docs: #doc is a sring, first tokenize it, and words in one sentence will only count once in df, will increase if in multiple docs\n",
    "        toks = set(simple_tokenize(doc))\n",
    "        for t in toks:\n",
    "            df_counter[t] += 1\n",
    "    items = [(tok, df) for tok, df in df_counter.items() if df >= min_df]\n",
    "    items.sort(key=lambda x: (-x[1], x[0])) #df decreasing order, dictionary increase order (lexicographic)\n",
    "    if max_vocab_size is not None:\n",
    "        items = items[:max_vocab_size]\n",
    "    vocab = {tok: i for i, (tok, _) in enumerate(items)}\n",
    "    return vocab, df_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c91edf-c073-46a6-ae17-c776f0defd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_count_matrix(docs, vocab):\n",
    "    \"\"\"\n",
    "    return shape: [n_docs, |V|]\n",
    "    document-term counts: every token frequency in each docs for this vocab\n",
    "    \"\"\"\n",
    "    n_docs = len(docs)\n",
    "    V = len(vocab)\n",
    "    X = np.zeros((n_docs, V), dtype=np.float32)\n",
    "    for i, doc in enumerate(docs):\n",
    "        toks = simple_tokenize(doc)\n",
    "        cnt = Counter(toks)\n",
    "        for tok, c in cnt.items():\n",
    "            j = vocab.get(tok, None)\n",
    "            if j is not None:\n",
    "                X[i, j] = c\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf51e621-3f56-425a-993d-88cdafe631dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_transform(counts):\n",
    "    \"\"\"\n",
    "    intput counts is the X returned by bow_count_matrix(docs, vocab)\n",
    "    TF = count / sum(counts_in_doc)\n",
    "    \"\"\"\n",
    "    row_sums = counts.sum(axis=1, keepdims=True) + 1e12\n",
    "    return counts / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e0f9f4-ee43-46c1-83db-ada5316a6975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_vector(docs, vocab, smooth=True):\n",
    "    \"\"\"\n",
    "    calculate IDF:\n",
    "    if not smooth: idf = log(N / df)\n",
    "    if smooth: idf = log((1 + N) / (1 + df)) + 1 sklearn style\n",
    "    \"\"\"\n",
    "    N = len(docs)\n",
    "    df = np.zeros(len(vocab), dtype=np.int32)\n",
    "    inv_vocab = {j : t for t, j in vocab.items()}\n",
    "    for j in range(len(vocab)):\n",
    "        tok = inv_vocab[j]\n",
    "        c = 0\n",
    "        for doc in docs:\n",
    "            if tok in set(simple_tokenize(doc)):\n",
    "                c += 1\n",
    "        df[j] = c\n",
    "    if smooth:\n",
    "        idf = np.log((1 + N) / (1 + df)) + 1.0\n",
    "    else:\n",
    "        idf = np.log(N / (df + 1e-12))\n",
    "    return idf.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ec5f6d8-2bbc-4ab8-8447-36f3f715ca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(docs, vocab, smooth=True):\n",
    "    counts = bow_count_matrix(docs, vocab) #shape: (N_docs, V)\n",
    "    TF = tf_transform(counts) #shape: (N_docs, V)\n",
    "    IDF = idf_vector(docs, vocab, smooth=smooth) #shape: (V,)\n",
    "    X = TF * IDF[None, :] #shape: (N_docs, V), IDF[None,:] will make (V,) to (1, V)\n",
    "    return X.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9472d4c3-2004-4c78-8ed5-143c0610dab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, df_counter = build_vocab(texts_train, min_df=1, max_vocab_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93539599-b6cd-44ac-ae89-2694d14930a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = bow_count_matrix(texts_train, vocab)\n",
    "X_test_bow = bow_count_matrix(texts_test, vocab)\n",
    "X_train_tfidf = tfidf_matrix(texts_train, vocab, smooth=True)\n",
    "X_test_tfidf = tfidf_matrix(texts_test, vocab, smooth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0ce3fd0-6276-455c-af5d-8ea55e3d060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scratch] Vocab size = 38\n",
      "[Scratch] BOW train shape = (5, 38), TF-IDF train shape = (5, 38)\n"
     ]
    }
   ],
   "source": [
    "print(f\"[Scratch] Vocab size = {len(vocab)}\")\n",
    "print(f\"[Scratch] BOW train shape = {X_train_bow.shape}, TF-IDF train shape = {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbb987b7-6e29-4bc1-b715-39b30a7c7688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Scratch] Top words by mean TF-IDF:\n",
      "  and              0.0000\n",
      "  a                0.0000\n",
      "  performances     0.0000\n",
      "  poor             0.0000\n",
      "  predictable      0.0000\n",
      "  recommend        0.0000\n",
      "  scenes           0.0000\n",
      "  script           0.0000\n",
      "  soundtrack       0.0000\n",
      "  strong           0.0000\n"
     ]
    }
   ],
   "source": [
    "#show highest weighted TF-IDF words\n",
    "mean_tfidf = X_train_tfidf.mean(axis=0)\n",
    "topk = np.argsort(-mean_tfidf)[:10]\n",
    "inv_vocab = {j: t for t, j in vocab.items()}\n",
    "top_words = [(inv_vocab[j], float(mean_tfidf[j])) for j in topk]\n",
    "print(\"[Scratch] Top words by mean TF-IDF:\")\n",
    "for w, s in top_words:\n",
    "    print(f\"  {w:15s}  {s:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc7cfd81-b8e7-4ddc-96e5-2cab3095c543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000000e-12, 8.4327911e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13, 4.1972247e-13, 4.1972247e-13,\n",
       "       4.1972247e-13, 4.1972247e-13], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33282f18-cc14-478c-8ece-8c5c876ee51d",
   "metadata": {},
   "source": [
    "# 2. Sklearn: Count & TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39616e3d-31bc-482b-b319-d9e59c8c8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c8e6467c-8afb-42db-acbc-c133b8c8e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, token_pattern=r\"[A-Za-z]+\")\n",
    "X_train_bow_sk = cv.fit_transform(texts_train)\n",
    "X_test_bow_sk = cv.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "599049d6-d038-431f-9e2a-beb8e6d455dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tv = TfidfVectorizer(lowercase=True, token_pattern=r\"[A-Za-z]+\", smooth_idf=True)\n",
    "X_train_tfidf_sk = tv.fit_transform(texts_train)\n",
    "X_test_tfidf_sk = tv.transform(texts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cceab776-6824-49dd-a0e3-c9a5c142f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sklearn] BOW train shape = (5, 38), TF-IDF train shape = (5, 38)\n"
     ]
    }
   ],
   "source": [
    "print(f\"[sklearn] BOW train shape = {X_train_bow_sk.shape}, TF-IDF train shape = {X_train_tfidf_sk.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1bb8eae1-25ae-4a42-86eb-8481ab7cc155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sklearn] LogReg on TF-IDF  Test Acc = 0.333\n"
     ]
    }
   ],
   "source": [
    "#simple BSL\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train_tfidf_sk, y_train)\n",
    "y_pred = clf.predict(X_test_tfidf_sk)\n",
    "print(f\"[sklearn] LogReg on TF-IDF  Test Acc = {accuracy_score(y_test, y_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9be16-f1d0-4145-b938-ac9ba0152994",
   "metadata": {},
   "source": [
    "# 3. PyTorch & TensorFlow on TF-IDF Features\n",
    "using the from scratch version TF-IDF for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b830181-a39e-496c-868d-128972799c7c",
   "metadata": {},
   "source": [
    "## 3.1 PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c1eb6df-0b2b-428e-bb0f-1953f49bf8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d615a2db-13af-49e1-baba-71ed3b15c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = X_train_tfidf.astype(np.float32)\n",
    "Xte = X_test_tfidf.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75963a6c-8555-4c59-b517-0fd545e2f04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch] ep=  1  train_loss=0.6849  test_acc=0.333\n",
      "[PyTorch] ep= 15  train_loss=0.6734  test_acc=0.333\n",
      "[PyTorch] ep= 30  train_loss=0.6734  test_acc=0.333\n",
      "[PyTorch] ep= 45  train_loss=0.6730  test_acc=0.333\n",
      "[PyTorch] ep= 60  train_loss=0.6730  test_acc=0.333\n"
     ]
    }
   ],
   "source": [
    "class TorchLogReg(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "model = TorchLogReg(in_dim=Xtr.shape[1]).to(device)\n",
    "opt = optim.Adam(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "Xtr_t = torch.tensor(Xtr, dtype=torch.float32, device=device)\n",
    "ytr_t = torch.tensor(y_train, dtype=torch.long, device=device)\n",
    "Xte_t = torch.tensor(Xte, dtype=torch.float32, device=device)\n",
    "yte_t = torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "for ep in range(1, epochs+1):\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    logits = model(Xtr_t)\n",
    "    loss = loss_fn(logits, ytr_t)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if ep % 15 == 0 or ep == 1:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            te_logits = model(Xte_t)\n",
    "            te_pred = te_logits.argmax(dim=1)\n",
    "            te_acc = (te_pred == yte_t).float().mean().item()\n",
    "        print(f\"[PyTorch] ep={ep:3d}  train_loss={loss.item():.4f}  test_acc={te_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6331ad5-2a80-4400-8652-8422828177bc",
   "metadata": {},
   "source": [
    "## 3.2 TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "472e71d2-7537-47df-86b6-75e7b3b6df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d257109-1aeb-479e-bdb3-c0d1e19a01f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-02 22:08:16.587706: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorFlow] Test Acc = 0.333\n"
     ]
    }
   ],
   "source": [
    "tf_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(Xtr.shape[1],)),\n",
    "    tf.keras.layers.Dense(2)\n",
    "])\n",
    "tf_model.compile(optimizer=tf.keras.optimizers.Adam(1e-2), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "hist = tf_model.fit(Xtr, y_train, validation_data=(Xte, y_test), batch_size=len(Xtr), epochs=60, verbose=0)\n",
    "te_loss, te_acc = tf_model.evaluate(Xte, y_test, verbose=0)\n",
    "print(f\"[TensorFlow] Test Acc = {te_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ebe7e-aef6-40f4-a8f9-eb45b6333c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
