{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9781388-8c55-48d2-8a71-743d66245444",
   "metadata": {},
   "source": [
    "# 1. TF-IDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da552302-29f8-4219-b3ba-fe9ff097f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple                  # type hints for clarity when reading function signatures\n",
    "import re                                       # regular expressions for custom tokenization\n",
    "import numpy as np                               # numerical arrays and vectorized operations\n",
    "\n",
    "# scikit-learn: feature extraction and modeling pieces\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # builds sparse TF-IDF features [N, V]\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS  # built-in English stopword set\n",
    "from sklearn.linear_model import LogisticRegression           # linear classifier baseline\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # evaluation metrics\n",
    "from sklearn.model_selection import train_test_split          # stratified train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b7ca5b-97ba-4dc1-8df9-e2115304804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a single string into lowercase alphabetic tokens and drop English stopwords.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    text : str\n",
    "        Raw input string (scalar).\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    tokens : List[str]\n",
    "        Tokens after lowercasing and stopword removal. Length = #matched tokens (no tensor shape).\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Custom tokenizer for TF-IDF to control normalization and stopword removal.\n",
    "    \"\"\"\n",
    "    toks = re.findall(r\"[a-z]+\", text.lower())\n",
    "    toks = [t for t in toks if t not in ENGLISH_STOP_WORDS]\n",
    "    return toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a807adb-08d5-4445-8173-e4769ab3ce26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tfidf(texts: List[str]) -> Tuple[TfidfVectorizer, \"scipy.sparse.csr_matrix\"]:\n",
    "    \"\"\"\n",
    "    Fit a TF-IDF (Term Frequencyâ€“Inverse Document Frequency) vectorizer and transform texts.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    texts : List[str]\n",
    "        N documents; Python list length = N.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Fitted vectorizer holding vocabulary V and IDF weights.\n",
    "    X : scipy.sparse.csr_matrix\n",
    "        Sparse TF-IDF matrix with shape [N, V].\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Creates and fits a TF-IDF vectorizer using our custom tokenizer, then transforms the texts.\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        tokenizer=tokenize,      # use our custom tokenizer defined above\n",
    "        preprocessor=None,       # we handle preprocessing inside tokenize()\n",
    "        token_pattern=None,      # disable the default token pattern since we provide tokenizer\n",
    "        lowercase=True,          # redundant but harmless; tokenize() already lowercases\n",
    "        ngram_range=(1, 1),      # unigrams only for this baseline\n",
    "        sublinear_tf=True        # log-scale term frequencies helps with heavy-tailed counts\n",
    "    )\n",
    "    \n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    return vectorizer, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "346f06a3-0e3e-41df-935d-30a6c6592397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logreg(X, y) -> LogisticRegression:\n",
    "    \"\"\"\n",
    "    Train a Logistic Regression classifier on TF-IDF features.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    X : scipy.sparse.csr_matrix\n",
    "        Feature matrix with shape [N, V].\n",
    "    y : np.ndarray\n",
    "        Integer labels with shape [N] (values in {0, 1, ..., C-1}).\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    model : LogisticRegression\n",
    "        Fitted scikit-learn Logistic Regression model.\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Learn a linear decision boundary on top of sparse TF-IDF features.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = LogisticRegression(max_iter=2000)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04be780e-8098-4ea9-bab6-0c80f2fd4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X, y) -> Tuple[float, str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Evaluate classifier: accuracy, per-class report, and confusion matrix.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model : LogisticRegression\n",
    "        Trained classifier.\n",
    "    X : scipy.sparse.csr_matrix\n",
    "        Feature matrix with shape [N, V].\n",
    "    y : np.ndarray\n",
    "        True labels with shape [N].\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    acc : float\n",
    "        Accuracy in [0, 1].\n",
    "    report : str\n",
    "        Text classification report (precision/recall/F1 per class).\n",
    "    cm : np.ndarray\n",
    "        Confusion matrix with shape [C, C].\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Standard validation routine for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    y_pred = model.predict(X)\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    report = classification_report(y, y_pred)\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    return acc, report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32f845e2-4a14-4916-b426-f955670a621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_texts(model, vectorizer, texts) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Predict class IDs and probabilities for raw texts.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model : LogisticRegression\n",
    "        Trained classifier.\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Fitted vectorizer to transform raw inputs the same way as training.\n",
    "    texts : List[str]\n",
    "        M new documents.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    pred : np.ndarray\n",
    "        Predicted class IDs, shape [M].\n",
    "    proba : np.ndarray\n",
    "        Predicted probabilities, shape [M, C].\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    One-stop prediction helper for new raw texts.\n",
    "    \"\"\"\n",
    "\n",
    "    X_new = vectorizer.transform(texts)\n",
    "    proba = model.predict_proba(X_new)\n",
    "    pred = np.argmax(proba, axis=1)\n",
    "    return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "050639a8-cd05-4e41-8a3d-e546f98c113f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Practice1] Valid accuracy = 0.333\n",
      "[Practice1] Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.33      1.00      0.50         1\n",
      "\n",
      "    accuracy                           0.33         3\n",
      "   macro avg       0.17      0.50      0.25         3\n",
      "weighted avg       0.11      0.33      0.17         3\n",
      "\n",
      "[Practice1] Confusion matrix:\n",
      " [[0 2]\n",
      " [0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mamba/Documents/MOOC/ML/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mamba/Documents/MOOC/ML/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/mamba/Documents/MOOC/ML/env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"This movie is great and excellent\",\n",
    "    \"Fantastic film with wonderful direction\",\n",
    "    \"Good plot and amazing soundtrack\",\n",
    "    \"Touching story with strong performances\",\n",
    "    \"Brilliant engaging narrative overall\",\n",
    "    \"This movie is bad and the pacing is awful\",\n",
    "    \"The film is boring with dull characters\",\n",
    "    \"Terrible editing and horrible dialogue\",\n",
    "    \"A predictable script with poor scenes\",\n",
    "    \"Unwatchable messy scenes and weak plot\",\n",
    "]\n",
    "labels = np.array([1,1,1,1,1, 0,0,0,0,0], dtype=np.int64)  # 1=positive, 0=negative\n",
    "\n",
    "\n",
    "X_train_txt, X_valid_txt, y_train, y_valid = train_test_split(corpus, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "tfv, X_tr = build_tfidf(X_train_txt) #shape [N_train, V]\n",
    "X_va = tfv.transform(X_valid_txt) #shape [N_valid, V]\n",
    "\n",
    "clf = train_logreg(X_tr, y_train)\n",
    "acc, rep, cm = evaluate(clf, X_va, y_valid)\n",
    "print(f\"[Practice1] Valid accuracy = {acc:.3f}\")\n",
    "print(\"[Practice1] Classification report:\\n\", rep)\n",
    "print(\"[Practice1] Confusion matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f879c3f5-eb51-4c65-8d8a-761f862c8145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Practice1] Predictions: [1 1]\n",
      "[Practice1] Probabilities:\n",
      " [[0.378 0.622]\n",
      " [0.492 0.508]]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"this film is wonderful and touching\",\n",
    "    \"awful boring movie with dull characters\"\n",
    "]\n",
    "pred, proba = predict_texts(clf, tfv, test_texts)\n",
    "print(\"[Practice1] Predictions:\", pred)\n",
    "print(\"[Practice1] Probabilities:\\n\", np.round(proba, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34604937-ec62-4a08-9c50-00ce0acc9284",
   "metadata": {},
   "source": [
    "# 2. Fusion: DocEmb = TF-IDF x Word2Vec (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f0350b7-be48-4431-b0a1-796eae994a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple                       # type hints\n",
    "import re                                            # regular expressions for tokenization\n",
    "import numpy as np                                   # numerical arrays and linear algebra\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF features (sparse)\n",
    "from sklearn.metrics import accuracy_score                    # basic evaluation metric\n",
    "from sklearn.linear_model import LogisticRegression           # linear classifier baseline\n",
    "\n",
    "try:\n",
    "    from gensim.models import Word2Vec             # Gensim's Word2Vec trainer (CBOW/Skip-gram with Negative Sampling)\n",
    "except Exception as e:\n",
    "    Word2Vec = None\n",
    "    print(\"[Practice2] gensim not available; please `pip install gensim` to run this section.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "792288b8-7c5e-438d-9d16-c181eef3c3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Tokenize a single string into lowercase alphabetic tokens.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    text : str\n",
    "        Raw input string.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    tokens : List[str]\n",
    "        List of tokens (variable-length list).\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Consistent tokenizer used by both TF-IDF and Word2Vec training.\n",
    "    \"\"\"\n",
    "    return re.findall(r\"[a-z]+\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62abaeeb-c362-4066-abd0-f56185cdf9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_w2v(sentences: List[List[str]],\n",
    "              vector_size: int = 100,\n",
    "              window: int = 5,\n",
    "              min_count: int = 1,\n",
    "              sg: int = 1,\n",
    "              negative: int = 5,\n",
    "              epochs: int = 20):\n",
    "    \"\"\"\n",
    "    Train a Word2Vec (word-to-vector) model with Gensim.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    sentences : List[List[str]]\n",
    "        Tokenized corpus (outer length = N sentences; inner lists have variable lengths).\n",
    "    vector_size : int\n",
    "        Embedding (vector) dimension d.\n",
    "    window : int\n",
    "        Context window size for co-occurrence.\n",
    "    min_count : int\n",
    "        Low-frequency cutoff; words with freq < min_count are dropped.\n",
    "    sg : int\n",
    "        1 = Skip-gram (center predicts context), 0 = CBOW (context predicts center).\n",
    "    negative : int\n",
    "        Number of negative samples per positive pair (Negative Sampling).\n",
    "    epochs : int\n",
    "        Training epochs over the corpus.\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    model : gensim.models.Word2Vec\n",
    "        Trained Word2Vec model (vectors accessible by model.wv).\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Learn dense word embeddings that capture distributional semantics.\n",
    "    \"\"\"\n",
    "\n",
    "    if Word2Vec is None:\n",
    "        raise RuntimeError(\"gensim is not installed.\")\n",
    "\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        sg=sg,\n",
    "        negative=negative,\n",
    "        hs=0, #use negative sampling (hierarchical softmax off)\n",
    "        sample=1e-3,\n",
    "        workers=2,\n",
    "        epochs=epochs,\n",
    "        seed=42\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5851827-7157-4cee-b2ec-fa061f6b19c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docemb_from_tfidf_w2v(tfv: TfidfVectorizer, X: \"scipy.sparse.csr_matrix\", wv_tokens: List[str], wv_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build dense document embeddings as: DocEmb = TF-IDF Ã— Embedding.\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    tfv : TfidfVectorizer\n",
    "        Fitted TF-IDF vectorizer, holding token->column mapping.\n",
    "    X : scipy.sparse.csr_matrix\n",
    "        TF-IDF feature matrix with shape [N, V_tfv].\n",
    "    wv_tokens : List[str]\n",
    "        Word2Vec vocabulary tokens aligned to rows in wv_matrix; length = V_w2v.\n",
    "    wv_matrix : np.ndarray\n",
    "        Word2Vec embedding matrix with shape [V_w2v, d].\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    DocEmb : np.ndarray\n",
    "        Dense document embeddings with shape [N, d].\n",
    "\n",
    "    Purpose\n",
    "    -------\n",
    "    Align the TF-IDF vocabulary with the Word2Vec vocabulary and compute a TF-IDF\n",
    "    weighted average of word vectors via matrix multiplication.\n",
    "    \"\"\"\n",
    "\n",
    "    tfv_vocab = tfv.vocabulary_                            #dict: token -> column\n",
    "    wv_index = {w: i for i, w in enumerate(wv_tokens)}     #dict: token -> row\n",
    "\n",
    "    common = sorted(set(tfv_vocab.keys()) & set(wv_index.keys()))\n",
    "    if not common:\n",
    "        raise ValueError(\"No overlapping tokens between TF-IDF vocab and Word2Vec vocab\")\n",
    "\n",
    "    cols = np.array([tfv_vocab[t] for t in common], dtype=int)\n",
    "    rows = np.array([wv_index[t] for t in common], dtype=int)\n",
    "    X_sub = X[:, cols] #[N, C]\n",
    "    W_sub = wv_matrix[rows, :] #[C, d]\n",
    "\n",
    "    DocEmb = X_sub @ W_sub\n",
    "    DocEmb = np.asarray(DocEmb)\n",
    "\n",
    "    norms = np.linalg.norm(DocEmb, axis=1, keepdims=True) + 1e-9\n",
    "    DocEmb = DocEmb / norms\n",
    "    return DocEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4ca7b93-4a0a-4fbb-8b7a-e543998b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This movie is great and excellent\",\n",
    "    \"Fantastic film with wonderful direction\",\n",
    "    \"Good plot and amazing soundtrack\",\n",
    "    \"Touching story with strong performances\",\n",
    "    \"Brilliant engaging narrative overall\",\n",
    "    \"This movie is bad and the pacing is awful\",\n",
    "    \"The film is boring with dull characters\",\n",
    "    \"Terrible editing and horrible dialogue\",\n",
    "    \"A predictable script with poor scenes\",\n",
    "    \"Unwatchable messy scenes and weak plot\",\n",
    "]\n",
    "labels = np.array([1,1,1,1,1, 0,0,0,0,0], dtype=np.int64)\n",
    "\n",
    "sentences = [tokenize(s) for s in corpus]\n",
    "w2v = train_w2v(sentences, vector_size=100, window=5, min_count=1, sg=1, negative=5, epochs=20)\n",
    "wv = w2v.wv\n",
    "wv_tokens = list(wv.key_to_index.keys())\n",
    "wv_matrix = wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "217fb4f4-200a-4a49-acd8-f576d64cc94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(tokenizer=tokenize, preprocessor=None, token_pattern=None, lowercase=True, ngram_range=(1,1), sublinear_tf=True)\n",
    "X = tfv.fit_transform(corpus)\n",
    "\n",
    "DocEmb = docemb_from_tfidf_w2v(tfv, X, wv_tokens, wv_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "741525c1-c555-4c1c-b22f-2b2bc509cf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Practice2] Accuracy on DocEmb (TF-IDF Ã— Word2Vec) = 1.000\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=2000)\n",
    "clf.fit(DocEmb, labels)\n",
    "pred = clf.predict(DocEmb)\n",
    "acc = accuracy_score(labels, pred)\n",
    "print(f\"[Practice2] Accuracy on DocEmb (TF-IDF Ã— Word2Vec) = {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c661dcff-6c97-4b4e-92cc-60dc2d357643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Practice2] Predictions: [1 0]\n"
     ]
    }
   ],
   "source": [
    "test_texts = [\n",
    "    \"this film is wonderful and touching\",\n",
    "    \"awful boring movie with dull characters\"\n",
    "]\n",
    "X_test = tfv.transform(test_texts)                              # [M, V_tfv]\n",
    "DocEmb_test = docemb_from_tfidf_w2v(tfv, X_test, wv_tokens, wv_matrix)  # [M, d]\n",
    "pred_test = clf.predict(DocEmb_test)\n",
    "print(\"[Practice2] Predictions:\", pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed7f466-aa9c-4c9a-803e-1792000860b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
